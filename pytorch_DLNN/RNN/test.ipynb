{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b55a292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e08e41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'love', 'deep', 'learning'], ['i', 'love', 'nlp'], ['pytorch', 'is', 'amazing', 'for', 'nlp']]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"I love deep learning\",\n",
    "    \"I love NLP\",\n",
    "    \"PyTorch is amazing for NLP\"\n",
    "]\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "tokenized_corpus = [tokenizer(sentence) for sentence in corpus]\n",
    "print(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59fdd792",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator( tokenized_corpus, specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "105a0649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 10\n",
      "Word -> Index mapping: {'pytorch': 9, 'learning': 8, 'is': 7, 'for': 6, 'deep': 5, 'amazing': 4, 'nlp': 3, 'i': 1, 'love': 2, '<unk>': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab size:\", len(vocab))\n",
    "print(\"Word -> Index mapping:\", vocab.get_stoi())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cb1599c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numericalized corpus: [[1, 2, 5, 8], [1, 2, 3], [9, 7, 4, 6, 3]]\n"
     ]
    }
   ],
   "source": [
    "numericalized = [vocab(tokens) for tokens in tokenized_corpus]\n",
    "print(\"Numericalized corpus:\", numericalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba532941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded shape: torch.Size([4, 50])\n",
      "tensor([[-0.0500,  0.6587,  1.1648, -1.0693,  0.3712, -0.2256, -1.1257,  0.3748,\n",
      "          0.4174,  0.1633,  1.6521,  0.6422,  0.3396,  0.6409, -0.6120, -1.5655,\n",
      "          0.1120,  0.8527, -0.8310,  0.2124, -0.8584,  0.7755,  1.0168, -0.5937,\n",
      "         -1.2423, -1.2420,  0.6951, -0.1970, -1.3777,  0.1003, -0.0777, -1.9960,\n",
      "          0.2769,  0.0621,  0.2846,  1.0130, -1.5226, -0.0774,  0.5942,  0.7252,\n",
      "         -0.1557, -1.1899,  0.8966,  0.5493, -0.0288,  1.4236, -0.3089, -2.8745,\n",
      "          0.4513, -0.2730],\n",
      "        [ 1.8075, -0.2100,  1.6299, -1.9432, -1.4332, -0.5559, -0.1015, -0.0674,\n",
      "          0.5600,  0.0114,  0.4599,  0.0794,  0.4144,  0.7365,  2.0062,  0.4038,\n",
      "          0.5327, -0.3421, -0.1681,  0.7718, -0.8312, -0.3992,  0.5420, -1.0114,\n",
      "          1.4819, -0.3148, -0.1770,  1.0229, -0.3627, -0.8726, -1.0074, -0.9971,\n",
      "         -0.7400, -0.0162,  0.3356,  0.9134,  0.3517, -1.8829,  1.1414,  0.7288,\n",
      "         -0.9678, -0.1095,  0.4044, -1.9440,  1.1365,  0.1724, -0.2606,  0.4643,\n",
      "         -0.4639,  0.1549],\n",
      "        [ 0.1060,  0.7220, -0.0422,  1.6307, -0.9831, -0.6215, -0.7995, -0.1170,\n",
      "         -0.0474,  0.0238, -1.0076, -0.4166, -0.3673, -0.4275,  0.9482, -2.6574,\n",
      "         -1.1290,  0.6286,  1.7667,  0.4708,  0.1353,  0.5277, -2.6703, -0.7959,\n",
      "          2.3510,  0.2024, -0.3488, -0.4761, -1.1399,  0.0154, -0.0698,  0.8258,\n",
      "         -0.9299,  1.4280, -0.2505, -0.7238,  0.5453,  0.6873, -2.4450,  1.5386,\n",
      "          0.5538, -1.4230, -0.0871, -0.7787, -0.8577, -1.0672, -1.6703,  0.2782,\n",
      "          0.6281,  0.3091],\n",
      "        [-0.0361, -1.3589,  0.1203,  2.1978,  0.0499, -0.7850, -0.4681, -0.1288,\n",
      "          0.6765, -0.1498,  0.6268,  2.3443, -0.8457, -0.2735, -0.6386,  0.8027,\n",
      "          0.0991,  0.8588,  0.3012, -1.7418, -1.9954, -0.2245,  1.4220, -0.1255,\n",
      "          1.0967, -0.2510,  0.0194,  0.8604,  0.6781, -0.6180,  1.3036,  0.0435,\n",
      "          1.2156,  0.3624, -0.2841,  1.4342, -0.9942, -0.1059, -0.4570, -0.6958,\n",
      "         -2.0497,  1.3670, -0.8945, -0.3086,  0.9407,  0.2518, -0.5093,  0.7493,\n",
      "          1.2979,  0.0675]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "embedding = torch.nn.Embedding(num_embeddings=len(vocab), embedding_dim=embedding_dim)\n",
    "\n",
    "# Convert one sentence to tensor\n",
    "sentence_tensor = torch.tensor(numericalized[0])  # \"i love deep learning\"\n",
    "embedded_sentence = embedding(sentence_tensor)\n",
    "\n",
    "print(\"Embedded shape:\", embedded_sentence.shape)  # [sentence_length, embedding_dim]\n",
    "print(embedded_sentence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
